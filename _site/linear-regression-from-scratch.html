<!DOCTYPE html>
<html lang="en">
    <head>
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">

        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Linear Regression from Scratch [CS]</title>
        <link rel="icon" type="image/png" href="img/favicon.png">
        <link rel="stylesheet" href="/assets/navbarStyles.css">
        <link rel="stylesheet" href="/assets/homeStyles.css">
        <link rel="stylesheet" href="/assets/blogStyles.css">
        <link rel="stylesheet" href="/assets/projectStyles.css">
        <link rel="stylesheet" href="/assets/postStyles.css">
        <link rel="stylesheet" href="/assets/contactStyles.css">
        <link rel="stylesheet" href="/assets/syntax.css">

        <script src="/navbarControl.js" defer></script>

        <link rel="stylesheet" href="/projects/circularMotion/circularMotionStyles.css">
        <link rel="stylesheet" href="/projects/gravitySimulator/gravitySimulatorStyles.css">
        <link rel="stylesheet" href="/projects/collisionDetection/collisionDetectionStyles.css">
        <link rel="stylesheet" href="/projects/lightTrails/lightTrailsStyles.css">
        <link rel="stylesheet" href="/projects/sinWaves/sinWavesStyles.css">
    </head>

    <body>
        <nav class="navbar">
            <div><a class="brand-title" href="/index.html">David Wang</a></div>
            <a href="#" class="toggle-button">
                <span class="bar"></span>
                <span class="bar"></span>
                <span class="bar"></span>
            </a>
            <div class="navbar-links">
                <ul>
                    <li><a href="/blog.html">Blog</a></li>
                    <li><a href="/projects.html">Projects</a></li>
                    <li><a href="/contact.html">Contact</a></li>
                </ul>
            </div>
        </nav>
        <script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>

<div class="container">

  <div class="postTitle"> Linear Regression From Scratch </div>
  <div class="desc"> Creating a Linear Regression Machine Learning Model from Scratch in Python </div>
  <div class="postDate"> Posted by David Wang on March 31, 2023 </div>

  <h2 id="introduction">Introduction</h2>

  <p>Linear Regresion is a machine learning model that aims to draw a best-fitted straight line through a set of data points. The goal is to use this line to describe the relationship between the input variables (independent variables) and the output variable (dependent variable).</p>

  <p>While machine learning libraries like scikit-learn can find the optimal regression line (as i have used <a href="https://github.com/davidw102403/linear_regression_scikit">here</a>), it won’t provide understanding of the underlying mathematics of the model. Implementing the model from scratch in python will allow us to gain a deeper understanding of how the algorithm works.</p>

  <h2 id="the-math">The Math</h2>

  <p>The basic framework of linear regression follows a linear function with structure:</p>

\[y = m \cdot x + b\]

  <p>The gradient, denoted by <strong><em>m</em></strong>, and the intercept, denoted by <strong><em>b</em></strong>, are the two unknown parameters that we need to determine in linear regression. To solve for these parameters, we can use a loss function and the gradient descent algorithm.</p>

  <h3 id="loss-function---mean-squared-error">Loss Function - Mean Squared Error</h3>

  <p>To find the values of <strong><em>m</em></strong> and <strong><em>b</em></strong>, we need to know how far each data point is from the regression line. This error can be calculated with a loss function. In this case, the mean squared error loss function can be used to calculate the distance between the regression line and the data point.</p>

  <p><img src="../images/error.png" alt="Alt text" /></p>

  <p>Using the mean squared error loss function, we take the distance between the data points and the regression line, square it, add it up for all data points, and then divide by the number of data points. In other words, we are taking the mean of the sum of the squared errors:</p>

\[E = \frac{1}{n} \cdot \sum_{i=1}^{n} (y_i - \bar{y}_i)^2\]

  <p>substituting in the equation of the regression line, we get:</p>

\[E = \frac{1}{n} \cdot \sum_{i=1}^{n} (y_i - (m \cdot x_i + b))^2\]

  <p>We can now find the regression line of best fit by minimizing this function for error.</p>

  <h3 id="gradient-descent">Gradient Descent</h3>

  <p>Gradient descent works by iteratively adjusting the model parameters in the direction of steepest descent of the error function. At each iteration, the algorithm calculates the gradient of the error function with respect to the model parameters. The algorithm then updates the model parameters by taking a step in the negative gradient direction.</p>

  <p>Think of it like walking down a hill to find the lowest point. You take steps in the direction that leads you downhill, until you reach the bottom. In the same way, gradient descent takes steps in the direction of decreasing error, until it reaches the lowest possible value of the error function.</p>

  <p>The size of the step is determined by the learning rate hyperparameter, which controls how far the algorithm moves in the negative gradient direction at each iteration. If the learning rate is too large, the algorithm may overshoot the minimum of the error function and fail to converge. If the learning rate is too small, the algorithm may take a long time to converge.</p>

  <p>The learning rate is like the size of the steps you take. If you take steps that are too big, you might overshoot the minimum point and end up climbing up the hill again. If you take steps that are too small, it might take a long time to get to the bottom of the hill. So, the learning rate has to be chosen carefully to balance between speed and accuracy.</p>

  <p>Taking the partial derivative of the error function with respect to <strong><em>m</em></strong> and <strong><em>b</em></strong>:</p>

\[\frac{\partial E}{\partial m} = -\frac{2}{n} \sum_{i=1}^{n} x_i \cdot (y - (m \cdot x_i + b))\]

\[\frac{\partial E}{\partial b} = -\frac{2}{n} \sum_{i=1}^{n} (y - (m \cdot x_i + b))\]

  <p>Scaling the gradient by learning rate <strong><em>L</em></strong> tells us how much we need to change the parameters. Since the gradient gives the direction of steepest increase, the negative gradient gives the direction of steepest decrease, which is what we want (to minimize the error function). Applying the changes at each interation:</p>

\[m = m - L \cdot \frac{\partial E}{\partial m}\]

\[b = b - L \cdot \frac{\partial E}{\partial b}\]

  <p>And thats it. We have successfully defined an algorithm to find parameters for <strong><em>m</em></strong> and <strong><em>b</em></strong>, which we can use to find the linear regression line. Now all that’s left is to implement it in code.</p>

  <h2 id="implementing-linear-regression-in-python">Implementing Linear Regression In Python</h2>

  <h3 id="loss-function-and-gradient-descent">Loss Function and Gradient Descent</h3>

  <p>We will be using Pandas to load the data and Matplotlib to visualize the results.</p>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div>  </div>

  <p>Next, load in a csv file with data (x-values and y-values).</p>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="s">'data.csv'</span><span class="p">)</span>
</code></pre></div>  </div>

  <p>We can start by defining the loss function.</p>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="n">total_error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">total_error</span> <span class="o">+=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="p">(</span><span class="n">m</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">total_error</span> <span class="o">/</span> <span class="nf">float</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
</code></pre></div>  </div>

  <p>The function takes in the parameters <em>m</em>, <em>b</em>, and the <em>data</em> points. It then loops through the number of rows/the number of data points. Using the <em>iloc</em> function, we extract each x and y coordinate from each row of the data (each data point). Then we apply the mean squared error function for each itteration. Finally, we return mean of the total error.</p>

  <p>The next step is to define the gradient descent function.</p>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">m_now</span><span class="p">,</span> <span class="n">b_now</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">L</span><span class="p">):</span>
  <span class="n">m_gradient</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">b_gradient</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">n</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">m_gradient</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="p">(</span><span class="n">m_now</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b_now</span><span class="p">))</span>
    <span class="n">b_gradient</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="p">(</span><span class="n">m_now</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b_now</span><span class="p">))</span>
  <span class="n">m</span> <span class="o">=</span> <span class="n">m_now</span> <span class="o">-</span> <span class="n">L</span> <span class="o">*</span> <span class="n">m_gradient</span>
  <span class="n">b</span> <span class="o">=</span> <span class="n">b_now</span> <span class="o">-</span> <span class="n">L</span> <span class="o">*</span> <span class="n">b_gradient</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">]</span>
</code></pre></div>  </div>

  <p>The function takes in parameters <em>m_now</em>, <em>b_now</em>, <em>data</em>, and <em>L</em>, representing the current m value, current b value, data points, and step size. It then loops through each data point, calculating and updating the gradient of the error function with respect to m and b. It then updates the final values of m and b that are used in the regression line, returning them in a list. The step size, <em>L</em>, determines the factor by which we change <em>m</em> and <em>b</em>.</p>

  <h3 id="training-the-model">Training the Model</h3>

  <p>To train the model, start by setting <em>m</em> and <em>b</em> both to zero, <em>L = 0.00001</em> and a variable <em>epochs = 100</em>.</p>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">m</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">L</span> <span class="o">=</span> <span class="mf">0.00001</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">L</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div>  </div>

  <p><em>Epochs</em> defines how many times the gradient_descent function will be applied or how many “steps”. Starting from zero, each “step” updates the values of <em>m</em> and <em>b</em> until the error function is minimized and the line of best fit is acheived.</p>

  <p>the results of <em>m</em> and <em>b</em>:</p>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mf">1.4567543102446447</span> <span class="mf">0.027979592525960988</span>
</code></pre></div>  </div>

  <p>We can easily use this model to make predictions by defining a simple method:</p>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">x_input</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">m</span> <span class="o">*</span> <span class="n">x_input</span> <span class="o">+</span> <span class="n">b</span>
</code></pre></div>  </div>

  <h3 id="plotting-the-results">Plotting the Results</h3>

  <p>To visualize the linear regression, we will plot both the calculated regression line and the error function using matplotlib. Start by creating two subplots.</p>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="s">"Linear Regression Model"</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="s">"Loss Function vs Time"</span><span class="p">)</span>
</code></pre></div>  </div>

  <p>Then we set the axis limits for the loss function plot. The function runs from 0 to epochs, so we’ll set that to be the limits on x. Since the goal is to visualize how the error decreases, we can set the y-limits from 0 to the initial value of the loss function. Then, plot the data points on the first subplot.</p>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ax2</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">])</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">data</span><span class="p">)])</span>

<span class="n">ax1</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
</code></pre></div>  </div>

  <p>Define a starting line for the regression model and a starting point for the loss function.</p>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">line1</span><span class="p">,</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">80</span><span class="p">),</span> <span class="nf">range</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">80</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">)</span>
<span class="n">line2</span><span class="p">,</span> <span class="o">=</span> <span class="n">ax2</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">)</span>
</code></pre></div>  </div>

  <p>Apply the gradient descent function for each iteration to calculate the values for m and b and set the y-values of the regression model to reflect the calculated line of best fit. At the same time, apply the loss function to calculate the error for each iteration, appending the values to a list for y values. Similarly, append each i value to a list of x values (reflecting increasing time). Finally, to visualize how error changes over time, set both the x_data and y_data of the loss function plot to these lists of changing values.</p>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xlist</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">ylist</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">L</span><span class="p">)</span>
    <span class="n">line1</span><span class="p">.</span><span class="nf">set_ydata</span><span class="p">(</span><span class="n">m</span> <span class="o">*</span> <span class="nf">range</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

    <span class="n">xlist</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">ylist</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">loss_function</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">data</span><span class="p">))</span>
    <span class="n">line2</span><span class="p">.</span><span class="nf">set_xdata</span><span class="p">(</span><span class="n">xlist</span><span class="p">)</span>
    <span class="n">line2</span><span class="p">.</span><span class="nf">set_ydata</span><span class="p">(</span><span class="n">ylist</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div>  </div>

  <p>The final result:</p>

  <p><img src="../images/regression_plot.png" alt="Alt text" /></p>

  <p><a href="https://github.com/davidw102403/LinearRegressionFromScratch">code</a></p>

</div>

    </body>
</html>